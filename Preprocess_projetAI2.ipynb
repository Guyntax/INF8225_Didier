{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocess_projetAI2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UlFn4lBJn9C"
      },
      "source": [
        "# Première version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-ZP-oXdl0Ly",
        "outputId": "2edf6915-88b3-457d-bf02-61c6eb181dbb"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "!pip install torchtext==0.6.0\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "import numpy as np\n",
        "import spacy\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
        "\n",
        "from matplotlib import pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.19.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.8.1+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.9.1\n",
            "    Uninstalling torchtext-0.9.1:\n",
            "      Successfully uninstalled torchtext-0.9.1\n",
            "Successfully installed sentencepiece-0.1.95 torchtext-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM6cS6P8namX",
        "outputId": "aa735613-cd16-4cae-fdf7-1b634c5abbea"
      },
      "source": [
        "\"\"\"\n",
        "Tokenizer et Vocabulaire\n",
        "\"\"\"\n",
        "\n",
        "!pip3 install spacy\n",
        "\n",
        "!python3 -m spacy download fr\n",
        "!python3 -m spacy download de\n",
        "!python3 -m spacy download en\n",
        "\n",
        "\n",
        "\n",
        "#src_exts = \"de\"\n",
        "src_exts = \"fr\"\n",
        "trg_exts = \"en\"\n",
        "\n",
        "\n",
        "\n",
        "spacy_src = spacy.load(src_exts)\n",
        "spacy_trg = spacy.load(\"en\")\n",
        "\n",
        "def tokenize_src(text):\n",
        "    return [tok.text for tok in spacy_src.tokenizer(text)]\n",
        "\n",
        "def tokenize_trg(text):\n",
        "    return [tok.text for tok in spacy_trg.tokenizer(text)]\n",
        "\n",
        "source_vocab = Field(tokenize=tokenize_src, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "target_vocab = Field(tokenize=tokenize_trg, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (56.0.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
            "Collecting fr_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz (14.7MB)\n",
            "\u001b[K     |████████████████████████████████| 14.7MB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from fr_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (56.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.10.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: fr-core-news-sm\n",
            "  Building wheel for fr-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fr-core-news-sm: filename=fr_core_news_sm-2.2.5-cp37-none-any.whl size=14727027 sha256=840d8eac4d3922d173b507df8706c22b848e37ebc93ad48bd00ede59ccf8a831\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8m983r7g/wheels/46/1b/e6/29b020e3f9420a24c3f463343afe5136aaaf955dbc9e46dfc5\n",
            "Successfully built fr-core-news-sm\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/fr_core_news_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/fr\n",
            "You can now load the model via spacy.load('fr')\n",
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (56.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp37-none-any.whl size=14907057 sha256=17895b07e3f30284327eb5680a3cf6d6db7f43846f3b10216136752cfce03e40\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qa1l42zl/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (56.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZxQ5JbqQMX3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "5373082c-eaea-4f70-e829-832f4d0541a0"
      },
      "source": [
        "\"\"\"\n",
        "Load donnée via le import DE-EN\n",
        "\"\"\"\n",
        "\n",
        "'''\n",
        "if src_exts == \"fr\":\n",
        "  train_data, valid_data, test_data = Multi30k.splits(\n",
        "      exts=(\".fr\", \".en\"), fields=(source_vocab, target_vocab)\n",
        "  )\n",
        "\n",
        "  source_vocab.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "  target_vocab.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nif src_exts == \"fr\":\\n  train_data, valid_data, test_data = Multi30k.splits(\\n      exts=(\".fr\", \".en\"), fields=(source_vocab, target_vocab)\\n  )\\n\\n  source_vocab.build_vocab(train_data, max_size=10000, min_freq=2)\\n  target_vocab.build_vocab(train_data, max_size=10000, min_freq=2)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW3ZnN8bDa2F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSne68eZ79DZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "31adaa21-7d3c-4379-8e69-b243f4056484"
      },
      "source": [
        "\"\"\"\n",
        "Load donnée via le GIT FR-EN\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "!git clone https://github.com/multi30k/dataset\n",
        "!pwd\n",
        "%cd dataset/data/task1/raw\n",
        "!ls\n",
        "\n",
        "!gzip  -d val.fr.gz\n",
        "!gzip  -d train.fr.gz\n",
        "!gzip  -d test_2016_flickr.fr.gz\n",
        "\n",
        "\n",
        "!gzip  -d val.en.gz\n",
        "!gzip  -d train.en.gz\n",
        "!gzip  -d test_2016_flickr.en.gz\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if src_exts == \"fr\":\n",
        "  train_src = open('train.fr', encoding='utf-8').read().split('\\n')\n",
        "  val_src = open('val.fr', encoding='utf-8').read().split('\\n')\n",
        "  test_src = open('test_2016_flickr.fr', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "  train_trg = open('train.en', encoding='utf-8').read().split('\\n')\n",
        "  val_trg = open('val.en', encoding='utf-8').read().split('\\n')\n",
        "  test_trg = open('test_2016_flickr.en', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "\n",
        "  \n",
        "  import pandas as pd\n",
        "\n",
        "  def data2df(src,trg):\n",
        "      raw_data = {'Source' : [line for line in src], 'Target': [line for line in trg]}\n",
        "      df = pd.DataFrame(raw_data, columns=[\"Source\", \"Target\"])\n",
        "      # remove very long sentences and sentences where translations are \n",
        "      # not of roughly equal length\n",
        "      df['src_len'] = df['Source'].str.count(' ')\n",
        "      df['trg_len'] = df['Target'].str.count(' ')\n",
        "      df = df.query('src_len < 80 & trg_len < 80')\n",
        "      df = df.query('src_len < trg_len * 1.5  &  trg_len <  src_len * 1.5 ')\n",
        "      return df \n",
        "\n",
        "  train_df = data2df(train_src, train_trg)\n",
        "  valid_df = data2df(val_src, val_trg)\n",
        "  test_df = data2df(test_src, test_trg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  from torch.utils.data.dataset import Dataset\n",
        "\n",
        "  import torchtext\n",
        "  source_vocab.is_target  = False\n",
        "  target_vocab.is_target  = True\n",
        "    \n",
        "  class QuackDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "          self.src = data[0]\n",
        "          self.trg = data[1]\n",
        "          \n",
        "          field_dict = {}\n",
        "          field_dict['src'] = source_vocab\n",
        "          field_dict['trg'] = target_vocab\n",
        "          setattr(self, \"fields\", field_dict)\n",
        "\n",
        "          data = list(zip(*data))\n",
        "          self.examples = [None]*len(data)\n",
        "          for idx, d in enumerate(data):\n",
        "            ex = torchtext.data.example.Example()\n",
        "            setattr(ex, \"src\", d[0])\n",
        "            setattr(ex, \"trg\", d[1])\n",
        "            self.examples[idx] = ex\n",
        "          \n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    #def __getitem__(self, idx):\n",
        "    #  return self.examples[idx]\n",
        "\n",
        "    def __iter__(self):\n",
        "      return QuackIterator(self)\n",
        "\n",
        "    def __getitem__(self, start, stop=None, step=None):\n",
        "\n",
        "        if stop == None and step == None:\n",
        "          idx = start\n",
        "          return self.examples[idx]\n",
        "\n",
        "        index = start\n",
        "        if stop == None:\n",
        "            end = start + 1\n",
        "        else:\n",
        "            end = stop\n",
        "        if step == None:\n",
        "            stride = 1\n",
        "        else:\n",
        "            stride = step\n",
        "\n",
        "        x = QuackDataset(self.examples[index:end:stride])\n",
        "        return x\n",
        "\n",
        "\n",
        "  class QuackIterator:\n",
        "    ''' Iterator class '''\n",
        "    def __init__(self,quackDataset):\n",
        "\n",
        "        print(type(quackDataset))\n",
        "\n",
        "        # object reference\n",
        "        self.quackDataset = quackDataset\n",
        "        # member variable to keep track of current index\n",
        "        self._index = 0\n",
        "    def __next__(self):\n",
        "\n",
        "        '''Returns the next value from team object's lists '''\n",
        "        if self._index < len(self.quackDataset.examples):\n",
        "          result = self.quackDataset[self._index]\n",
        "          self._index +=1\n",
        "          return result\n",
        "\n",
        "        # End of Iteration\n",
        "        raise StopIteration\n",
        "\n",
        "  train_list = [[source_vocab.preprocess(item) for item in list(train_df['Source'])],[target_vocab.preprocess(item) for item in list(train_df['Target'])]]\n",
        "  valid_list = [[source_vocab.preprocess(item) for item in list(valid_df['Source'])],[target_vocab.preprocess(item) for item in list(valid_df['Target'])]]\n",
        "  test_list = [[source_vocab.preprocess(item) for item in list(test_df['Source'])],[target_vocab.preprocess(item) for item in list(test_df['Target'])] ]\n",
        "\n",
        "  train_data = QuackDataset(train_list)\n",
        "  valid_data = QuackDataset(valid_list)\n",
        "  test_data  = QuackDataset(test_list)\n",
        "\n",
        "\n",
        "  def flipSourceData(datasets):\n",
        "    for i,data in enumerate(datasets):\n",
        "      for j,example in enumerate(data.examples):\n",
        "        (datasets[i]).examples[j].src = (example.src)[::-1]\n",
        "    return datasets\n",
        "\n",
        "  #train_data,valid_data, test_data = flipSourceData(train_data,valid_data, test_data )\n",
        "\n",
        "  isinstance(train_data, Dataset)\n",
        "\n",
        "  from collections import Counter, OrderedDict\n",
        "  def build_vocab_quack(vocab_field,field, *args, **kwargs):\n",
        "          '''Construct the Vocab object for this field from one or more datasets.\n",
        "\n",
        "          Arguments:\n",
        "              Positional arguments: Dataset objects or other iterable data\n",
        "                  sources from which to construct the Vocab object that\n",
        "                  represents the set of possible values for this field. If\n",
        "                  a Dataset object is provided, all columns corresponding\n",
        "                  to this field are used; individual columns can also be\n",
        "                  provided directly.\n",
        "              Remaining keyword arguments: Passed to the constructor of Vocab.\n",
        "          '''\n",
        "          counter = Counter()\n",
        "          sources = []\n",
        "          for arg in args:\n",
        "              if isinstance(arg, Dataset):\n",
        "                  sources += [getattr(arg, field)]\n",
        "\n",
        "\n",
        "              else:\n",
        "                  sources.append(arg)\n",
        "          \n",
        "          for data in sources:\n",
        "              for x in data:\n",
        "                  if not vocab_field.sequential:\n",
        "                      x = [x]\n",
        "                  try:\n",
        "                      counter.update(x)\n",
        "                  except TypeError:\n",
        "                      counter.update(chain.from_iterable(x))\n",
        "          specials = list(OrderedDict.fromkeys(\n",
        "              tok for tok in [vocab_field.unk_token, vocab_field.pad_token, vocab_field.init_token,\n",
        "                              vocab_field.eos_token] + kwargs.pop('specials', [])\n",
        "              if tok is not None))\n",
        "          vocab_field.vocab = vocab_field.vocab_cls(counter, specials=specials, **kwargs)\n",
        "          return vocab_field\n",
        "\n",
        "  source_vocab = build_vocab_quack(source_vocab,'src',train_data, max_size=10000, min_freq=2)\n",
        "  target_vocab = build_vocab_quack(target_vocab,'trg',train_data, max_size=10000, min_freq=2)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'dataset'...\n",
            "remote: Enumerating objects: 304, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 304 (delta 11), reused 25 (delta 7), pack-reused 262\u001b[K\n",
            "Receiving objects: 100% (304/304), 18.60 MiB | 31.69 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n",
            "/content\n",
            "/content/dataset/data/task1/raw\n",
            "test_2016_flickr.cs.gz\ttest_2017_mscoco.en.gz\ttrain.en.gz\n",
            "test_2016_flickr.de.gz\ttest_2017_mscoco.fr.gz\ttrain.fr.gz\n",
            "test_2016_flickr.en.gz\ttest_2018_flickr.cs.gz\tval.cs.gz\n",
            "test_2016_flickr.fr.gz\ttest_2018_flickr.de.gz\tval.de.gz\n",
            "test_2017_flickr.de.gz\ttest_2018_flickr.en.gz\tval.en.gz\n",
            "test_2017_flickr.en.gz\ttest_2018_flickr.fr.gz\tval.fr.gz\n",
            "test_2017_flickr.fr.gz\ttrain.cs.gz\n",
            "test_2017_mscoco.de.gz\ttrain.de.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f6ec43889732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m   \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflipSourceData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m   \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: flipSourceData() takes 1 positional argument but 3 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "WY2lPZSPn_8B",
        "outputId": "435a5ae7-2c3a-43dc-f7e1-c9ed23dce8df"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d2c601a7036d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msource_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab_quack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_vocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtarget_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab_quack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_vocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'trg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'build_vocab_quack' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I50JlGxOHr9W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "6f4dacee-a0b3-4030-9644-b12331789e34"
      },
      "source": [
        "\n",
        "field_dict = {}\n",
        "field_dict['src'] = source_vocab\n",
        "field_dict['trg'] = target_vocab\n",
        "\n",
        "\n",
        "train_data = Dataset(train_list,field_dict)\n",
        "valid_data = Dataset(valid_list,field_dict)\n",
        "test_data  = Dataset(test_list,field_dict)\n",
        "\n",
        "isinstance(train_data, Dataset)\n",
        "\n",
        "from collections import Counter, OrderedDict\n",
        "def build_vocab_quack(vocab_field,field, *args, **kwargs):\n",
        "        '''Construct the Vocab object for this field from one or more datasets.\n",
        "\n",
        "        Arguments:\n",
        "            Positional arguments: Dataset objects or other iterable data\n",
        "                sources from which to construct the Vocab object that\n",
        "                represents the set of possible values for this field. If\n",
        "                a Dataset object is provided, all columns corresponding\n",
        "                to this field are used; individual columns can also be\n",
        "                provided directly.\n",
        "            Remaining keyword arguments: Passed to the constructor of Vocab.\n",
        "        '''\n",
        "        counter = Counter()\n",
        "        sources = []\n",
        "        for arg in args:\n",
        "            if isinstance(arg, Dataset):\n",
        "                sources += [getattr(arg, field)]\n",
        "\n",
        "\n",
        "            else:\n",
        "                sources.append(arg)\n",
        "        \n",
        "        for data in sources:\n",
        "            for x in data:\n",
        "                if not vocab_field.sequential:\n",
        "                    x = [x]\n",
        "                try:\n",
        "                    counter.update(x)\n",
        "                except TypeError:\n",
        "                    counter.update(chain.from_iterable(x))\n",
        "        specials = list(OrderedDict.fromkeys(\n",
        "            tok for tok in [vocab_field.unk_token, vocab_field.pad_token, vocab_field.init_token,\n",
        "                            vocab_field.eos_token] + kwargs.pop('specials', [])\n",
        "            if tok is not None))\n",
        "        vocab_field.vocab = vocab_field.vocab_cls(counter, specials=specials, **kwargs)\n",
        "        return vocab_field\n",
        "\n",
        "source_vocab = build_vocab_quack(source_vocab,'src',train_data, max_size=10000, min_freq=2)\n",
        "target_vocab = build_vocab_quack(target_vocab,'trg',train_data, max_size=10000, min_freq=2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-fb175e567c0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfield_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mvalid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfield_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtest_data\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfield_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/typing.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, *args, **kwds)\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object.__new__() takes exactly one argument (the type to instantiate)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "063DCqqATPrR"
      },
      "source": [
        "%cd /content\n",
        "\n",
        "torch.save(source_vocab, \"source.pt\")\n",
        "torch.save(target_vocab, \"target.pt\")\n",
        "\n",
        "torch.save(train_data.fields, \"train_fields.pt\")\n",
        "torch.save(train_data.examples, \"train_ex.pt\")\n",
        "torch.save(valid_data.fields, \"valid_fields.pt\")\n",
        "torch.save(valid_data.examples, \"valid_ex.pt\")\n",
        "torch.save(test_data.fields, \"test_fields.pt\")\n",
        "torch.save(test_data.examples, \"test_ex.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBoDEgk1DvQb"
      },
      "source": [
        "% cd /content\n",
        "!pwd\n",
        "!ls\n",
        "!git clone https://github.com/Guyntax/INF8225_Didier\n",
        "% cd /content/INF8225_Didier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoVYe0b4jcJJ"
      },
      "source": [
        "from torchtext.data import Dataset\n",
        "\n",
        "reload_source_vocab = torch.load(\"source.pt\")\n",
        "reload_target_vocab = torch.load(\"target.pt\")\n",
        "\n",
        "re_train_data_fields = torch.load(\"train_fields.pt\")\n",
        "re_train_data_examples = torch.load(\"train_ex.pt\")\n",
        "re_valid_data_fields = torch.load(\"valid_fields.pt\")\n",
        "re_valid_data_examples = torch.load(\"valid_ex.pt\")\n",
        "re_test_data_fields = torch.load(\"test_fields.pt\")\n",
        "re_test_data_examples = torch.load(\"test_ex.pt\")\n",
        "\n",
        "reload_train_data = Dataset(re_train_data_examples, re_train_data_fields)\n",
        "reload_valid_data = Dataset(re_valid_data_examples, re_valid_data_fields)\n",
        "reload_test_data = Dataset(re_test_data_examples, re_test_data_fields)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36ZKUO4BBWTP"
      },
      "source": [
        "\"\"\"\n",
        "def save_examples(dataset, savepath):\n",
        "    with open(savepath, 'w') as f:\n",
        "\n",
        "        # Save elements\n",
        "        for pair in dataset.examples:\n",
        "            data = [pair.English, pair.French]\n",
        "            f.write(json.dumps(data))  # Write samples\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "\n",
        "def load_examples(filename):\n",
        "    examples = []\n",
        "    with open(filename, 'r') as f:\n",
        "        # Read num. elements (not really need it)\n",
        "        total = json.loads(f.readline())\n",
        "\n",
        "        # Save elements\n",
        "        for i in range(total):\n",
        "            line = f.readline()\n",
        "            example = json.loads(line)\n",
        "            # example = data.Example().fromlist(example, fields)  # Create Example obj. (you can do it here or later)\n",
        "            examples.append(example)\n",
        "\n",
        "    end = time.time()\n",
        "    print(end - start)\n",
        "    return examples\n",
        "\n",
        "\n",
        "import json\n",
        "examples = load_examples('train.json')\n",
        "examples = [data.Example().fromlist(d, data_fields) for d in examples]\n",
        "train = Dataset(examples, data_fields)\n",
        "\n",
        "# Reloading the validation dataset\n",
        "examples = load_examples('val.json')\n",
        "examples = [data.Example().fromlist(d, data_fields) for d in examples]\n",
        "val = Dataset(examples, data_fields)\n",
        "\n",
        "# # Reloading the test dataset\n",
        "# examples = load_examples(test.json)\n",
        "# examples = [data.Example().fromlist(d, data_fields) for d in examples]\n",
        "# test = Dataset(examples, data_fields)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}